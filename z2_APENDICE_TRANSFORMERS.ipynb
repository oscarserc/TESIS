{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTO8k23wt5K3FKf1dhCTQj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscarserc/TESIS/blob/main/z2_APENDICE_TRANSFORMERS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Análisis del funcionamiento del TRANSFORMER**\n",
        "\n",
        "Este tipo de arquitectura neuronal está especialmente diseñada para el análisis de secuencias, y se originó en el área de Aprendizaje de Lenguaje Natural (NLP - *Natural Language Processing*). Una de sus bondades reside en que son capaces de detectar dependencias muy complejas, y entre elementos que están muy separados entre sí. En modelos previos (GRU, LSTM, etc.) aparecen serias dificultades cuando se procesa texto y las oraciones son muy largas, y existen relaciones de este tipo. El Transformer es especialmente hábil para sortear estos obstáculos.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Texto_1a.png?raw=true' width=\"600\"/>\n",
        "<figcaption>fig. 1a: Vemos que en una oración, las palabras que están relacionadas entre sí, pueden estar separadas por distancias muy variables. Por ejemplo 'perro' y 'gato' aparecen casi juntas así como el motivo por el que se establece la relación entre ellos: ambos son animales de compañía. Sin embargo la mención a algún otro animal por el mismo motivo, puede aparecer al final de un párrafo muy largo. También el verbo final hace alusión al inicio de la frase. Este tipo de dependencias son muy dificiles de detectar por muchas arquitecturas neuronales.</figcaption></center>\n",
        "</figure>\n",
        "<br></br>\n",
        "\n",
        "Para poder tratar información como es un fragmento de texto se han de seguir una serie de pasos previos (*figura 1b* ): primero se divide la frase en fragmentos (llamemoslos: '<b>tokens</b>') como podrían ser las palabras, por ejemplo. Cada uno de los ellos puede tener una codificación numérica. Posteriormente se procede a proyectar cada uno de estos elementos en un espacio de alta dimensionalidad. Llamemos D al valor de esta dimensión (en aplicaciones prácticas se suele utilizar D=768). Esto se traduce en que ahora cada fragmento de la frase pasa a ser representado por un vector con D componentes. El motivo para utilizar un valor tan alto en aplicaciones del lenguaje se debe al elevado número de palabras que puede aparecer en un texto, así como a la dificultad añadida de que cada palabra puede tener varios significados.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Tokenizaci%C3%B3n_1e.png?raw=true' width=\"700\"/>\n",
        "<figcaption>fig. 1b: Proceso de fragmentación de un texto en 'tokens' y la posterior proyección de cada uno de ellos en un espacio de elevada dimensión (en aplicaciones del lenguaje se suele tomar un valor D=768, aunque pueden utilizarse valores mucho más altos para tareas más complejas). Vemos las diferentes representaciones de la información inicial. Al principio es sólo texto. Luego tras la división en tokens, cada palabra queda representada por un código numérico, y finalmente por una proyección D-dimensional.</font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Para que una red neuronal sea capaz de realizar tareas como traducir o llevar a cabo una conversación, será necesario incorporar el contexto de tal manera que las palabras que estén relacionadas (por tener un significado similar por ejemplo) estén cercanas entre sí en el espacio D-dimensional sobre el que las hemos proyectado.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Embedding_1a.png?raw=true' width=\"700\"/>\n",
        "<figcaption>fig. 1c: Tras la proyección de los tokens en un espacio de dimensión D, convendrá aproximar entre sí a los elementos que tienen algún tipo de relación entre ellos. En el gráfico vemos una parte de este espacio (de dimensión $d_{h}$) y el acercamiento de las coordenadas de varias de las palabras del ejemplo. (<i>En aras de la comprensión, la representación aparece en un espacio euclideo, aunque podría no serlo. Todo dependerá de cómo definamos la similitud entre tokens</i> )</font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "El resultado del procesamiento podría ser el que aparece en la figura 1c. De esta tarea se encarga una parte del Transformer: el ENCODER. El entrenamiento de la habilidad particular en que habrá de especializarse el Transformer (resumir textos, traducir,etc.) se lleva a cabo en el DECODER, empleando como parte de los inputs de entrenamiento esta proyección contextual (***embedding*** ) generada como output por el encoder.\n",
        "\n",
        "\n",
        "Todos los modelos del lenguaje avanzados (LLM - *Large Language Models*), como ChatGPT por poner un ejemplo, están basados en este tipo de arquitectura. Algunas modelos sólo requieren el ENCODER, otros pueden funcionar con el DECODER simplemente y otros tienen ambas estructuras.\n",
        "\n",
        "Si acoplamos estas dos partes del Transformer, la circulación de los datos adopta la siguiente estructura esquemática general (*figura 1d* ).\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Estructura_Transformer_2.png?raw=true' width=\"350\"/>\n",
        "<figcaption>fig. 1d: Esquema de la arquitectura Transformer, en este caso procesando una frase y llevando a cabo su traducción. (Aparece la estructura desplegada completa. En gris la parte ENCODER y en color rosa la parte DECODER) (Adaptado de <font color='blue'>[Lane, H., & Dyshel, M. (2025)]). </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "\n",
        "#**<font color='brown'>· <u>Clasificación</u>.</font>**\n",
        "\n",
        "Las tareas en que puede ser entrenado un Transformer van desde hacer\n",
        "resúmenes de texto, hasta traducciones, pasando por la improvisación\n",
        "de respuestas y la redacción de textos.\n",
        "Dependiendo de la naturaleza del objetivo que buscamos, su arquitectura\n",
        "variará.\n",
        "\n",
        "\n",
        "En tareas que sólo requieren una representación suficientemente rica\n",
        "de los inputs de entrada puede ser suficiente el disponer sólo de la\n",
        "parte ENCODER. Y en otros casos podemos encontrar sólo la parte de\n",
        "procesamiento que se lleva en el DECODER\n",
        "Los modelos más exitosos que se han desarrollado se pueden resumir,\n",
        "en función de esta distinción, tal y como aparece a continuación.\n",
        "\n",
        "1) ***Sólo ENCODER***: BERT, DistilBERT, RoBERTa. Permiten capturar la información\n",
        "semántica que contiene el texto por ejemplo. Con imágenes podemos encontrar\n",
        "ViT (Visual Transformer) que es un clasificador de imagenes.\n",
        "\n",
        "2) ***Sólo DECODER***: GPT-2 o Llama, entre otros. Su especialidad es la de generar\n",
        "contenido.\n",
        "\n",
        "3) ***ENCODER-DECODER***: BART, T5. Se especializan en realizar resúmenes\n",
        "o traducciones (es decir, generan contenido como respuesta al input que se\n",
        "le ha pasado previamente y que han de tomar como referencia).\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Para tener una idea clara de las posibilidades que ofrece, así como el tipo\n",
        "de modificaciones que se pueden llevar a cabo para adaptar su funcionamiento\n",
        "a la tarea particular que pretendemos llevar a cabo, conviene analizar\n",
        "detenidamente las distintas partes del Transformer.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "#**<font color='brown'>· <u>Análisis de la estructura Encoder-Decoder</u>.</font>**\n",
        "\n",
        "En términos más concretos tenemos que interiormente los bloques ENCODER (de color gris a la izquierda) y los bloques DECODER (de color rosa a la derecha) de la figura anterior (fig. 1d) tienen la siguiente forma (fig. 2).\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Estructura_Transformer_1b.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 2: Esquema más detallado del interior de los bloques que procesan la información en el Transformer. Cada uno de ellos aparece replicado N veces (<i>Notado en la figura: Nx</i> ). (La parte izquierda es el ENCODER y la derecha el DECODER) (Adaptado de <font color='blue'>[Vasgani, A. et al. (2023)]). </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Si repasamos lo que hemos visto, vemos que la utilidad del ENCODER reside en lograr una representación sensible al contexto de la secuencia de INPUTS que recibe.\n",
        "Básicamente consiste en asociar a cada fragmento de información (token) una\n",
        "proyección (embedding) en un espacio de alta dimensionalidad, de tal manera\n",
        "que elementos similares tengan unas coordenadas parecidas y estén próximos\n",
        "entre sí. Para poder llevar a cabo la tarea computacionalmente compleja que se le exige, se utilizan varias cabezas de procesamiento, que analizan en paralelo las relaciones de similitud entre tokens y generando paso a paso embeddings acorde con el contexto. Esto se traduce en capturar el significado de la información de entrada. Para ello empleamos un mecanismo de atención.\n",
        "Posteriormente los datos son procesados por un DECODER, que es el encargado\n",
        "del aprendizaje de la tarea concreta que se desea desarrollar en el\n",
        "transformer.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Encoder_1f_Cuadros_mas_claros.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 3: Esquema del mecanismo de atención del Transformer. Entre sus características tenemos que es facilmente paralelizable. </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "La arquitectura del transformer le permite analizar\n",
        "secuencias de información muy largas e interiorizar dependencias\n",
        "con una estructura muy compleja.\n",
        "Esta característica es muy útil con cierto tipo de información\n",
        "como es el lenguaje natural (NLP - Natural Language Processing).\n",
        "Su origen se encuentra en este área de investigación, que es de\n",
        "donde partió, aunque se uso se ha extendendido a otros campos (como la\n",
        "visión por computadora) con resultados satisfactorios.\n",
        "\n",
        "Estructura de la Arquitectura TRANSFORMERS.\n",
        "<font color='blue'>[Vasgani, A. et al. (2023)]</font>\n",
        "\n",
        "Esquema y enlaces de la Arquitectura TRANSFORMERS.\n",
        "<font color='blue'>[Bahdanau, D. et al (2016)]</font>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "\n",
        "\n",
        "#**<font color='brown'>· <u>Mecanismos de atención</u>.</font>**\n",
        "\n",
        "El poder del transformer reside, entre otras cosas, en el mecanismo\n",
        "de atención que emplea, que le permite centrarse en aquellos elementos\n",
        "que realmente están relacionados entre sí y obviar el resto (En la figura 3 este bloque está en color naranja con el nombre '*Multi-Head Attention*')\n",
        "A su vez tiene una estructura que facilita la paralelización del\n",
        "procesamiento de datos (utilizando múltiples CPUs o GPUs) (ver fig. 4).\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Attention_1a.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 4: Esquema de la parte computacional del ENCODER del Transformer. La división en distintas cabezas hace que los cálculos sean facilmente paralelizables (Adaptado de <font color='blue'>[Vasgani, A. et al. (2023)] ). </font> </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Podemos analizar más detenidamente la forma de computar internamente de los mecanismos de multi-head attention (Ver cap. 9 - *Speech and Language Processing*. <font color='blue'>[Daniel Jurafsky & James H. Martin (2025)]</font>).\n",
        "\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/NLP_Transformers_1b.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 5: Esquema del mecanismo de atención del Transformer </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "**Ref**.:\n",
        "\n",
        "<font color='brown'>[Bahdanau, D. et al (2016)]</font>\n",
        "*Neural Machine Translation by Jointly Learning to Align and Translate*\n",
        "https://arxiv.org/pdf/1409.0473\n",
        "\n",
        "<font color='brown'>[Vasgani, A. et al. (2023)]</font>\n",
        "*Attention is All You Need* https://arxiv.org/pdf/1706.03762\n",
        "\n",
        "<font color='brown'>[Dosovitskiy, A. et al. (2021)] </font>\n",
        "*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale* https://arxiv.org/pdf/2010.11929\n",
        "\n",
        "<font color='brown'>[Caron, M. et al. (2021)]</font>\n",
        "*Emerging Properties in Self-Supervised Vision Transformers* https://arxiv.org/pdf/2104.14294\n",
        "\n",
        "<font color='brown'>[Lane, H., & Dyshel, M. (2025)]</font>\n",
        "*Natural language processing in action.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abBRG35CG_iK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxMGYu1AG_3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWB5JyzUG_87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Análisis computacional del ENCODER**\n",
        "\n",
        "El objetivo que nos proponíamos al inicio es la generación de una proyección que sea sensible al contexto y que se caracterice por reflejar las relaciones entre **tokens** en términos de relaciones de cercanía en el espacio sobre el que haremos la proyección.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Embedding_matrix_1a.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 6: Representación del objetivo que buscamos con el ENCODER: generar proyecciones espaciales sensibles al contexto. La matriz de entrada, $X_{T\\;x\\;d}$, tras ser procesada por esta parte del Transformer deberá generar una matriz transformada, con unas coordenadas próximas para los tokens que están relacionados entre sí. En la imagen vemos el caso particular del ejemplo, donde los <b>tokens</b>: $X_{3}, X_{6}, X_{T-3}$ pasan a estar cercanos en el subespacio representado. El Transformer es capaz de capturar relaciones semánticas como esta.</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "En la figura 7 podemos ver el proceso de transformación de las proyecciones iniciales, con el objetivo de ir aproximando los tokens que tienen características similares. (En color azul aparecen las partes donde se lleva a cabo aprendizaje durante la aplicación del algoritmo de entrenamiento).\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/zz_Matrices_1l_CPU.png?raw=true' width=\"750\"/>\n",
        "<figcaption>fig. 7: Representación esquemática del flujo de información dentro del ENCODER del Transformer. Como <b>input</b> tenemos una matriz que contiene todas las proyecciones d-dimensionales de los <b>T</b> tokens. Es una matriz $X_{T\\;x\\;d}$. Podemos ver que se produce un procesamiento independiente en paralelo, en <b>h</b> cabezas de procesamiento.</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "De manera análoga a como una red convolucional emplea varios <b>kernels</b>, para detectar diferentes rasgos o matices en las imágenes. Aquí tenemos que cada cabezal de procesamiento del ENCODER del Transformer necesita 3 matrices: <b>Q</b> (<i>query</i> ), <b>K</b> (<i>key</i> ), <b>V</b> (<i>value</i> ). La primera de ellas es similar a un kernel.\n",
        "\n",
        "\n",
        "<br></br>\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/zz_Matrices_1a2h_Ampliado.png?raw=true' width=\"750\"/>\n",
        "<figcaption>fig. 8: Vemos cómo se fragmenta el procesamiento, capacitando a que cada cabezal de atención analice información de una parte del espacio de proyección. Al multiplicar por la matriz de entrada  $X_{T\\;x\\;d}$, obrendremos las matrices <b>Q</b> (<i>query</i> ), <b>K</b> (<i>key</i> ), <b>V</b> (<i>value</i> ) en cada cabeza de entrenamiento.</figcaption></center>\n",
        "</figure>\n",
        "<br></br>\n",
        "\n",
        "\n",
        "En primer lugar conviene observar cómo se generan las matrices que se emplearán en cada cabezal para buscar tokens similares (ver figura 7). El mecanismo de atención nos permite encontrar pesos asociados a la mayor o menor similitud entre <b>tokens</b>, y con ellos combinar los vectores <b>V</b> para generar proyecciones cercanas para elementos similares. Los detalles de la computación relativa al mecanismo de atención lo podemos ver en la figura 8 que aparece a continuación.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/zz_Matrices_1a5_Recortado_a1c_PRUEBA.png?raw=true' width=\"850\"/>\n",
        "<figcaption>fig. 9: Esquema del mecanismo de atención del Transformer. Vemos la imagen completa de todo el proceso, particularizada en detalle en la primera cabeza de procesamiento. Empezamos con el <b>input</b> (matriz de entrada)  $X_{T\\;x\\;d}$, que tras llevar a cabo el producto con las matrices de proyección: $W^{i}_{Q},\\;W^{i}_{K},\\;W^{i}_{V}$ nos permite obtener las matrices <b>$Q_{\\;T\\;x\\;d_{h}}^{i}$</b> (<i>query</i> ), <b>$K_{\\;T\\;x\\;d_{h}}^{i}$</b> (<i>key</i> ), <b>$V_{\\;T\\;x\\;d_{h}}^{i}$</b> (<i>value</i> ) en cada cabeza de entrenamiento ($i=1,2,...,h$). La operación con las matrices resultantes: (<b><i>Softmax</i></b>$[QxK^{T}])/d_{h}^{1/2}$ tiene dimensiones $T\\;x\\;T$ y nos proporciona una puntuación (score) que refleja el grado de similitud entre los $T$ <b>tokens</b>. Al multiplicar por la matriz $V$ obtendremos una combinación lineal ponderada por dichos pesos, lo que conduce al acercamiento progresivo de las coordenadas en el espacio D-dimensional de los elementos más similares.</font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/zz_Matrices_2a.png?raw=true' width=\"850\"/>\n",
        "<figcaption>fig. 9a: La multiplicación de matrices conduce a unas dimensiones sensibles al contexto. </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "El procedimiento completo para todas las cabezas de procesamiento lo podemos observar a continuación (ver figura 9).\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/zz_Matrices_1a5_Recortado_a4.png?raw=true' width=\"850\"/>\n",
        "<figcaption>fig. 9b: Esquema del mecanismo de atención del Transformer con todos las cabezas de procesamiento. </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "<br></br>\n"
      ],
      "metadata": {
        "id": "PYCg-smFtg1v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crM28tP36S04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-BKjjtpthSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Análisis computacional mediante PyTorch**\n",
        "\n",
        "En la figura 10 podemos ver un conjunto de tensores para simular una matriz con las proyecciones (embeddings) de un conjunto de **tokens**.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Pytorch_1a.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 10: Utilizamos tensores de PyTorch. Creamos una matriz: <b>X</b> de dimensiones 5 x 12. Es decir 5 filas que corresponden a 5 <b>tokens</b>, cada uno de ellos de dimensión D=12. Para simplificar asignamos un único valor en todas las dimensiones, y hacemos completamente iguales dos de ellos. Podríamos procesar esta información con h = 4 cabezales (<i>heads</i>) de procesamiento (i=1,2,3,4), que procesarían vectores con una dimensión fracción de la dimensión D de partida: $d_{h}=3 \\;\\;(d_{h}=D/h=12/4=3)$.</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "El mecanismo de memoria que utiliza el Transformer primero tiene que fraccionar el espacio de proyección. Una vez llevado a cabo podemos paralelizar el código, y procesar en cada cabeza en busca de patrones y dependencias entre los tokens.\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Pytorch_2a.png?raw=true' width=\"800\"/>\n",
        "<figcaption>fig. 11: Utilizamos tensores de PyTorch. Creamos una matriz: <b>X</b> de dimensiones: 5 x 12. Es decir 5 filas que corresponden a <b>T </b>= 5 <b>tokens</b>, cada uno de ellos de dimensión D=12. Podríamos procesar esta información con 4 cabezales de procesamiento (h=1,2,3,4), que procesarían vectores con una dimensión fracción de la dimensión D: $d_{h}=3$.</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Luego será necesario recuperar la forma original para acoplar todas las piezas. En la siguiente figura podemos observar los mecanismos que proporciona PyTorch para intercambiar dimensiones y trasponer matrices.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Pytorch_3a.png?raw=true' width=\"700\"/>\n",
        "<figcaption>fig. 12: Un conjunto de operaciones nos permiten recuperar la forma original.</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HprzGboIbbSv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zaap23upbbht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBWtV4DWbbk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**TRANSFORMERS visuales**\n",
        "\n",
        "\n",
        "Los resultados de tratamiento de secuencias se acabaron de trasladar al procesamiento de imágenes.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Visual_Transformer_1a.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 6: Esquema de la arquitectura Visual Transformer (ViT) <font color='blue'>[Dosovitskiy, A. et al. (2021)].</font>En la figura, podemos ver que el ENCODER aparece replicado un número de L veces (notado Lx). </figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Algunos autores han comprobado que las features obtenidas a través de autoentrenamiento de un ViT (visual Transformer) contienen suficiente información explícita como para llevar a cabo directamente la segmentación de las imágenes <font color='blue'>[Caron, M. et al. (2021)]</font>.\n",
        "\n",
        "<br></br>\n"
      ],
      "metadata": {
        "id": "of7PgayvSWq3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nhAI2_VOHAC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ueNTvwAiHAGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}