{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMO1hjoYOiFqF/5wHSMz8/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscarserc/TESIS/blob/main/z2_APENDICE_TRANSFORMERS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Análisis del funcionamiento del TRANSFORMER**\n",
        "\n",
        "Este tipo de arquitectura neuronal está especialmente diseñada para el análisis de secuencias, y se originó en el área de Aprendizaje de Lenguaje Natural (NLP - *Natural Language Processing*). Todos los modelos del lenguaje avanzados (LLM - *Large Language Models*), como ChatGPT por poner un ejemplo, están basados en este tipo de arquitectura.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Estructura_Transformer_1b.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 1: Esquema de la arquitectura Transformer (aparece la estructura completa formada por un ENCODER y un DECODER) (Adaptado de <font color='blue'>[Vasgani, A. et al. (2023)]). </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Si estudiamos la forma en que está diseñado, podemos detectar una primera\n",
        "capa de procesamiento que es el ENCODER. Su utilidad reside en lograr\n",
        "una representación sensible al contexto de la secuencia de INPUTS que recibe.\n",
        "Básicamente consiste en asociar a cada fragmento de información (token) una\n",
        "proyección (embedding) en un espacio de alta dimensionalidad, de tal manera\n",
        "que elementos similares tengan unas coordenadas parecidas y estén próximos\n",
        "entre sí. Esto se traduce en capturar el significado de la información de\n",
        "entrada.\n",
        "Posteriormente los datos son procesados por un DECODER, que es el encargado\n",
        "del aprendizaje de la tarea concreta que se desea desarrollar en el\n",
        "transformer.\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Encoder_1f_Cuadros_mas_claros.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 3: Esquema del mecanismo de atención del Transformer. Entre sus características tenemos que es facilmente paralelizable. </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "La arquitectura del transformer le permite analizar\n",
        "secuencias de información muy largas e interiorizar dependencias\n",
        "con una estructura muy compleja.\n",
        "Esta característica es muy útil con cierto tipo de información\n",
        "como es el lenguaje natural (NLP - Natural Language Processing).\n",
        "Su origen se encuentra en este área de investigación, que es de\n",
        "donde partió, aunque se uso se ha extendendido a otros campos (como la\n",
        "visión por computadora) con resultados satisfactorios.\n",
        "\n",
        "Estructura de la Arquitectura TRANSFORMERS.\n",
        "<font color='blue'>[Vasgani, A. et al. (2023)]</font>\n",
        "\n",
        "Esquema y enlaces de la Arquitectura TRANSFORMERS.\n",
        "<font color='blue'>[Bahdanau, D. et al (2016)]</font>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Estructura_Transformer_1a.png?raw=true' width=\"450\"/>\n",
        "<figcaption>fig. 2: Esquema de la arquitectura Transformer (aparece la estructura completa formada por un ENCODER y un DECODER) <font color='blue'> [Vasgani, A. et al. (2023)]</font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "El poder del transformer reside, entre otras cosas, en el mecanismo\n",
        "de atención que emplea, que le permite centrarse en aquellos elementos\n",
        "que realmente están relacionados entre sí y obviar el resto.\n",
        "A su vez tiene una estructura que facilita la paralelización del\n",
        "procesamiento de datos (utilizando múltiples CPUs o GPUs) (ver fig. 2).\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Attention_1a.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 3: Esquema del mecanismo de atención del Transformer. Entre sus características tenemos que es facilmente paralelizable. </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Podemos analizar más detenidamente la forma de computar internamente de los mecanismos de multi-head attention (Ver cap. 9 - *Speech and Language Processing*. <font color='blue'>[Daniel Jurafsky & James H. Martin (2025)]</font>).\n",
        "\n",
        "\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/NLP_Transformers_1b.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 4: Esquema del mecanismo de atención del Transformer </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "#**<font color='brown'>· <u>Transformers Visuales</u>.</font>**\n",
        "\n",
        "\n",
        "Los resultados de tratamiento de secuencias se acabaron de trasladar al procesamiento de imágenes.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/oscarserc/IMAGENES/blob/main/Visual_Transformer_1a.png?raw=true' width=\"650\"/>\n",
        "<figcaption>fig. 5: Esquema de la arquitectura Visual Transformer (ViT) <font color='blue'>[Dosovitskiy, A. et al. (2021)] </font></figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Algunos autores han comprobado que las features obtenidas a través de autoentrenamiento de un ViT (visual Transformer) contienen suficiente información explícita como para llevar a cabo directamente la segmentación de las imágenes <font color='blue'>[Caron, M. et al. (2021)]</font>.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "#**<font color='brown'>· <u>Clasificación</u>.</font>**\n",
        "\n",
        "Las tareas en que puede ser entrenado un Transformer van desde hacer\n",
        "resúmenes de texto, hasta traducciones, pasando por la improvisación\n",
        "de respuestas y la redacción de textos.\n",
        "Dependiendo de la naturaleza del objetivo que buscamos, su arquitectura\n",
        "variará.\n",
        "\n",
        "\n",
        "En tareas que sólo requieren una representación suficientemente rica\n",
        "de los inputs de entrada puede ser suficiente el disponer sólo de la\n",
        "parte ENCODER. Y en otros casos podemos encontrar sólo la parte de\n",
        "procesamiento que se lleva en el DECODER\n",
        "Los modelos más exitosos que se han desarrollado se pueden resumir,\n",
        "en función de esta distinción, tal y como aparece a continuación.\n",
        "\n",
        "1) Sólo ENCODER: BERT, DistilBERT, RoBERTa. Permiten capturar la información\n",
        "semántica que contiene el texto por ejemplo. Con imágenes podemos encontrar\n",
        "ViT (Visual Transformer) que es un clasificador de imagenes.\n",
        "\n",
        "2) Sólo DECODER: GPT-2 o Llama, entre otros. Su especialidad es la de generar\n",
        "contenido.\n",
        "\n",
        "3) ENCODER-DECODER: BART, T5. Se especializan en realizar resúmenes\n",
        "o traducciones (es decir, generan contenido como respuesta al input que se\n",
        "le ha pasado previamente y que han de tomar como referencia).\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Para tener una idea clara de las posibilidades que ofrece, así como el tipo\n",
        "de modificaciones que se pueden llevar a cabo para adaptar su funcionamiento\n",
        "a la tarea particular que pretendemos llevar a cabo, conviene analizar\n",
        "detenidamente las distintas partes del Transformer.\n",
        "\n",
        "\n",
        "<br></br>\n",
        "**Ref**.:\n",
        "\n",
        "<font color='brown'>[Bahdanau, D. et al (2016)]</font>\n",
        "*Neural Machine Translation by Jointly Learning to Align and Translate*\n",
        "https://arxiv.org/pdf/1409.0473\n",
        "\n",
        "<font color='brown'>[Vasgani, A. et al. (2023)]</font>\n",
        "*Attention is All You Need* https://arxiv.org/pdf/1706.03762\n",
        "\n",
        "<font color='brown'>[Dosovitskiy, A. et al. (2021)] </font>\n",
        "*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale* https://arxiv.org/pdf/2010.11929\n",
        "\n",
        "<font color='brown'>[Caron, M. et al. (2021)]</font>\n",
        "*Emerging Properties in Self-Supervised Vision Transformers* https://arxiv.org/pdf/2104.14294\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abBRG35CG_iK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxMGYu1AG_3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWB5JyzUG_87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nhAI2_VOHAC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ueNTvwAiHAGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}